<img width="1641" height="709" alt="comp-gaps-banner" src="https://github.com/user-attachments/assets/880b3b6b-bdac-4343-aef2-c474a4f1fb5d" />

# Uncovering Competency Gaps in Large Language Models and Their Benchmarks

#### [Maty Bohacek](https://www.matybohacek.com), [Nino Scherrer](https://ninodimontalcino.github.io), [Nicholas Dufour](https://scholar.google.com/citations?user=g42kJfIAAAAJ), [Thomas Leung](https://scholar.google.com/citations?user=sUK_w2QAAAAJ), [Christoph Bregler](https://chris.bregler.com), [Stephanie C.Y. Chan](https://scychan.github.io/)

This repository contains the official implementation of Competency Gaps, a representation-grounded evaluation method that uses sparse autoencoders (SAEs) to automatically surface both model gaps and benchmark gaps. The approach extracts SAE concepts and computes saliency-weighted performance scores to reveal why models succeed or fail and which concepts benchmarks over- or under-represent. Applied to multiple open-source LLMs and benchmarks, the method recovers known weaknesses without supervision.

> Website — [Paper](https://competency-gaps.github.io/paper) — [Contact us](mailto:email@example.com)
>
> *Under review*

## Getting Started

Code coming soon.

## Citation

If you find our work useful, please consider citing our paper.

```bibtex
@inproceedings{tbd2026competencygaps,
  title={Uncovering Competency Gaps in Large Language Models and Their Benchmarks},
  author={TBD},
  booktitle={TBD},
  year={TBD}
}
